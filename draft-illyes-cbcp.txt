



Network Working Group                                          G. Illyes
Internet-Draft                                               Independent
Intended status: Informational                             M. Kuehlewind
Expires: 7 January 2026                                         Ericsson
                                                             6 July 2025


                         Crawler best practices
                        draft-illyes-cbcp-latest

Abstract

   This document describes best pratices for web crawlers.

Discussion Venues

   This note is to be removed before publishing as an RFC.

   Source for this draft and an issue tracker can be found at
   https://github.com/garyillyes/cbcp.

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at https://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on 7 January 2026.

Copyright Notice

   Copyright (c) 2025 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents (https://trustee.ietf.org/
   license-info) in effect on the date of publication of this document.
   Please review these documents carefully, as they describe your rights
   and restrictions with respect to this document.

Table of Contents

   1.  Introduction
   2.  Recommended Best Practices
     2.1.  Crawlers must respect the Robots Exclusion Protocol
     2.2.  Crawlers must be easily identifiable through their user
           agent string
     2.3.  Crawlers must not interfere with the normal operation of a
           site
     2.4.  Crawlers must support caching directives
     2.5.  Crawlers must expose the IP ranges they use for crawling
     2.6.  Crawlers must explain how the crawled data is used and the
           crawler can be blocked
   3.  Conventions and Definitions
   4.  Security Considerations
   5.  IANA Considerations
   6.  Normative References
   Acknowledgments
   Authors' Addresses

1.  Introduction

   Automatic clients (i.e. crawlers, bots) are used to access web
   resources, e.g., for indexing for search but more recently also for
   new use cases related to Artificial Intelligence (AI) such as
   training models.  With an increase in crawling activity, it is
   particular important that automatic clients having an expected
   behavior and respect contraint of the ressources being crawled.  This
   includes documenting how to identify them and how their behavior can
   be influenced.  As such, crawler operators are requested to conform
   to crawling best pratices in this document.

   To further help website owners, it should in addition be considered
   to create a central registry where website owners can look up well
   behaving crawlers.  Note that while self declared research crawlers,
   including privacy and malware discovery crawlers, and contractual
   crawlers are welcome to add themselves to adopt these practices, due
   to the nature of the relationship with sites they may exempt
   themselves from any of the Crawler Code of Conduct policies with a
   rationale.

2.  Recommended Best Practices

   The following best practices are should be followed and are already
   applied by a vast majority of large scale crawlers on the Internet:

   1.  Crawlers must support and respect the Robots Exclusion Protocol.

   2.  Crawlers must be easily identifiable through their user agent
       string.

   3.  Crawlers must not interfere with the normal operation of a site.

   4.  Crawlers must support caching directives.

   5.  Crawlers must expose the IP ranges they are crawling from in a
       standardized format.

   6.  Crawlers must expose a page where they explain how the crawled
       data is used and how it can be blocked.

2.1.  Crawlers must respect the Robots Exclusion Protocol

   All well behaved crawlers must support the REP as defined in
   Section 2.2.1 of [REP] to allow site owners to opt out from crawling.

   Especially if the website does not support REP, crawlers further need
   to respect the meta robots tag in the HTTP header.

2.2.  Crawlers must be easily identifiable through their user agent
      string

   As stipulated in Section 2.2.1 of [REP] (Robots Exclusion Protocol;
   REP), the HTTP request header user-agent should identify the crawler
   clearly, typically by including a URL that hosts the crawler's
   description.  For example:

   User-Agent: Mozilla/5.0 (compatible; ExampleBot/0.1;
   +https://www.example.com/bot.html).

   This is already a widely supported mechanism among crawler operators.
   To be compliant, crawler operators must specify identifiers unique
   for their crawlers within the case-insensitive user-agent, like
   "contains 'googlebot' and 'https://url/...'".  Further, the name
   should be meaningful in identifying the crawler owner and purpose as
   much as reasonable possible.

2.3.  Crawlers must not interfere with the normal operation of a site

   Depending on a site's setup (computing resources, software
   efficiency) and its size, crawling may slow down the site or take it
   offline altogether.  Crawler operators must ensure that their
   crawlers are equipped with back-out logic that rely on at least the
   standard signals defined by Section 15.6 of [HTTP-SEMANTICS],
   preferably also additional heuristics such as relative response time
   of the server.

   As such, crawlers should log already visited URL and how many
   requests they sent to a resource as well as the respective HTTP
   status codes in the responses, especially if error occur, to avoid
   repeatedly crawling the same source.

   Generally, crawlers should avoid multiple concurrent requests to the
   same resources and should limit the crawling rate to avoid overload,
   if possible considering limits specified in the REP protocol.
   Further, ressources should not be re-crawled too frequently.  Ideally
   crawlers should limit the crawling depth and number of requests per
   ressource to avoid loops.

   Crawlers should not try to circumvent authentication or other access
   restrictions e.g. when a login is required, CAPTCHAs are used, or the
   content is behind a paywall, except if explicitly negotiated with the
   website owner.

   Crawlers should access resources primarily with HTTP GET requests,
   resorting to other methods (e.g., POST, PUT) only if there is a
   preexisting agreement with the publisher or the publisher's content
   management software makes those calls automatically when JavaScript
   is executed.  Generally, load caused by executing JavaScript should
   be considered carefully or even be avoided whenever possible.

2.4.  Crawlers must support caching directives

   [HTTP-CACHING] HTTP caching, which removes the need of repeated
   access from crawlers to the same URL.

2.5.  Crawlers must expose the IP ranges they use for crawling

   To complement the REP, crawler operators should expose the IP ranges
   they allocated for crawling in a standardized, machine readable
   format, and keep it reasonably up to date (i.e. shouldn't get older
   than 7 days).

   The object containing the IP addresses must be linked from the page
   describing the crawler and it must be also referenced in the metadata
   of the page for machine readability.  For example:

   &lt;link rel="help" href="https://example.com/crawlerips.json" /&gt;

2.6.  Crawlers must explain how the crawled data is used and the crawler
      can be blocked

   Crawlers must be easily identifiable through their user agent string
   crawlers should explain how the data they crawled will be used.  In
   practice this is generally done through the documentation page
   referenced in the user-agent of the crawler.  Further the
   documentation page should provide a contact address for the crawler
   owner.

   The webpage should also provides an example REP file to block the
   crawler and a way to verify REP files.

3.  Conventions and Definitions

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in
   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all
   capitals, as shown here.

4.  Security Considerations

   TODO Security

5.  IANA Considerations

   This document has no IANA actions.

6.  Normative References

   [HTTP-CACHING]
              Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke,
              Ed., "HTTP Caching", STD 98, RFC 9111,
              DOI 10.17487/RFC9111, June 2022,
              <https://www.rfc-editor.org/rfc/rfc9111>.

   [HTTP-SEMANTICS]
              Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke,
              Ed., "HTTP Semantics", STD 97, RFC 9110,
              DOI 10.17487/RFC9110, June 2022,
              <https://www.rfc-editor.org/rfc/rfc9110>.

   [REP]      Koster, M., Illyes, G., Zeller, H., and L. Sassman,
              "Robots Exclusion Protocol", RFC 9309,
              DOI 10.17487/RFC9309, September 2022,
              <https://www.rfc-editor.org/rfc/rfc9309>.

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <https://www.rfc-editor.org/rfc/rfc2119>.

   [RFC8174]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC
              2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174,
              May 2017, <https://www.rfc-editor.org/rfc/rfc8174>.

Acknowledgments

   TODO acknowledge.

Authors' Addresses

   Gary Illyes
   Independent
   Email: synack@garyillyes.com


   Mirja KÃ¼hlewind
   Ericsson
   Email: mirja.kuehlewind@ericsson.com
